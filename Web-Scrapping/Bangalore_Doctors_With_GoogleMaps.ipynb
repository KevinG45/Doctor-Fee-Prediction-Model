{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bangalore Doctors Web Scraping with Google Maps Links\n",
    "\n",
    "This notebook scrapes doctor information from Practo for Bangalore only and includes Google Maps links for each doctor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_google_maps_link(doctor_name, location, city):\n",
    "    \"\"\"\n",
    "    Generate a Google Maps search link for a doctor based on their name and location\n",
    "    \"\"\"\n",
    "    # Clean and format the search query\n",
    "    search_query = f\"{doctor_name} doctor {location} {city}\"\n",
    "    # URL encode the search query\n",
    "    encoded_query = urllib.parse.quote_plus(search_query)\n",
    "    # Create Google Maps search URL\n",
    "    maps_url = f\"https://www.google.com/maps/search/{encoded_query}\"\n",
    "    return maps_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataframe with all columns including Google Maps link\n",
    "df = pd.DataFrame({\n",
    "    'Name': [''], \n",
    "    'Speciality': [''], \n",
    "    'Degree': [''], \n",
    "    'Year_of_experience': [''], \n",
    "    'Location': [''], \n",
    "    'City': [''], \n",
    "    'dp_score': [''], \n",
    "    'npv': [''], \n",
    "    'consultation_fee': [''],\n",
    "    'google_maps_link': ['']\n",
    "})\n",
    "\n",
    "# Focus only on Bangalore\n",
    "city = 'Bangalore'\n",
    "\n",
    "# All specialities as in original code\n",
    "Speciality = [\n",
    "    'Cardiologist', 'Chiropractor', 'Dentist', 'Dermatologist', \n",
    "    'Dietitian/Nutritionist', 'Gastroenterologist', 'bariatric surgeon', \n",
    "    'Gynecologist', 'Infertility Specialist', 'Neurologist', 'Neurosurgeon', \n",
    "    'Ophthalmologist', 'Orthopedist', 'Pediatrician', 'Physiotherapist', \n",
    "    'Psychiatrist', 'Pulmonologist', 'Rheumatologist', 'Urologist'\n",
    "]\n",
    "\n",
    "print(f\"Starting web scraping for {city} doctors...\")\n",
    "print(f\"Total specialities to process: {len(Speciality)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Chrome options for better performance\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in background\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Main scraping loop - focused only on Bangalore\n",
    "for j in Speciality:\n",
    "    print(f\"\\nProcessing {j} specialists in {city}...\")\n",
    "    \n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        url = f\"https://www.practo.com/search/doctors?results_type=doctor&q=%5B%7B%22word%22%3A%22{j}%22%2C%22autocompleted%22%3Atrue%2C%22category%22%3A%22subspeciality%22%7D%5D&city={city}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Scroll to load all doctors\n",
    "        scroll_pause_time = 2 \n",
    "        screen_height = driver.execute_script(\"return window.screen.height;\") \n",
    "        A = 1\n",
    "\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, {screen_height}*{A});\".format(screen_height=screen_height, A=A))\n",
    "            A += 1\n",
    "            time.sleep(scroll_pause_time)\n",
    "      \n",
    "            scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "       \n",
    "            if (screen_height) * A > scroll_height:\n",
    "                break\n",
    "     \n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        postings = soup.find_all('div', class_='u-border-general--bottom')\n",
    "        \n",
    "        doctors_found = 0\n",
    "        \n",
    "        for post in postings:\n",
    "            try:\n",
    "                link = post.find('div', class_='listing-doctor-card').find('a').get('href')\n",
    "                link_full = 'https://www.practo.com' + link\n",
    "                driver.get(link_full)\n",
    "                time.sleep(2)\n",
    "                soup2 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "                # Initialize variables\n",
    "                name = \"N/A\"\n",
    "                Degree = \"N/A\"\n",
    "                Year_of_experience = \"N/A\"\n",
    "                Location = \"N/A\"\n",
    "                dp_score = \"N/A\"\n",
    "                npv = \"N/A\"\n",
    "                consultant_fee = \"N/A\"\n",
    "                \n",
    "                # Extract doctor information\n",
    "                try:\n",
    "                    name = soup2.find('h1', class_='c-profile__title u-bold u-d-inlineblock').text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    Degree = soup2.find('p', class_='c-profile__details').text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    Year_of_experience = soup2.find('div', class_='c-profile__details').find_all('h2')[-1].text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    Location = soup2.find('h4', class_='c-profile--clinic__location').text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    dp_score = soup2.find('span', class_='u-green-text u-bold u-large-font').text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    npv = soup2.find('span', class_='u-smallest-font u-grey_3-text').text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    consultant_fee = soup2.find('span', class_='u-strike').text.strip()\n",
    "                except:\n",
    "                    try:\n",
    "                        consultant_fee = soup2.find('div', class_='u-f-right u-large-font u-bold u-valign--middle u-lheight-normal').text.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Generate Google Maps link\n",
    "                google_maps_link = generate_google_maps_link(name, Location, city)\n",
    "                \n",
    "                # Append data to dataframe using pd.concat instead of deprecated append\n",
    "                new_row = pd.DataFrame({\n",
    "                    'Name': [name], \n",
    "                    'Speciality': [j], \n",
    "                    'Degree': [Degree], \n",
    "                    'Year_of_experience': [Year_of_experience], \n",
    "                    'Location': [Location], \n",
    "                    'City': [city], \n",
    "                    'dp_score': [dp_score], \n",
    "                    'npv': [npv], \n",
    "                    'consultation_fee': [consultant_fee],\n",
    "                    'google_maps_link': [google_maps_link]\n",
    "                })\n",
    "                \n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "                doctors_found += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing doctor: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Found {doctors_found} {j} specialists in {city}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {j}: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # Small delay between specialities\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"\\nCompleted scraping! Total doctors found: {len(df) - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the initial empty row\n",
    "df = df[1:].reset_index(drop=True)\n",
    "\n",
    "# Display summary\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of Google Maps links\n",
    "print(\"Sample Google Maps links generated:\")\n",
    "for i in range(min(5, len(df))):\n",
    "    print(f\"{df.iloc[i]['Name']} - {df.iloc[i]['google_maps_link']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the enhanced dataset\n",
    "output_file = '../DATA/bangalore_doctors_with_maps.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nDataset saved to: {output_file}\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Columns included: {', '.join(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation and statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total doctors: {len(df)}\")\n",
    "print(f\"\\nSpeciality distribution:\")\n",
    "print(df['Speciality'].value_counts())\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}